# ReSpAct: Harmonizing Reasoning, Speaking, and Acting
[![Paper](https://img.shields.io/badge/arXiv-Paper-red.svg)]()

**[ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents](https://vardhandongre.github.io/respact-llm/)**  
[Vardhan Dongre](https://vardhandongre.github.io/), [Xiaocheng Yang](https://www.linkedin.com/in/xiaocheng-yang-1a68aa20b?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADVY_o4B3amA3qxReMk8okt2Vk9XjaRcS0g&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_all%3B%2FelJ2zYqRVuwEdBXgxKDEw%3D%3D), [Emre Can Acikgoz](https://emrecanacikgoz.github.io/), [Suvodip Dey](https://scholar.google.com/citations?user=cCFhUMwAAAAJ&hl=en), [Gokhan Tur](https://siebelschool.illinois.edu/about/people/faculty/gokhan), [Dilek Hakkani-TÃ¼r](https://siebelschool.illinois.edu/about/people/faculty/dilek)

<p float="left">
  <img src="assets/diagram.gif">
</p>

This repository contains code for reproducing results. If you find this work useful in your research, please cite:

```
@inproceedings{,
  bibtex_show = {true},
  title = {ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents},
  author = {},
  booktitle = {ArXiv},
  year = {preprint},
  html = {},
  tag = {NLP}
}
```

## Quickstart

Create a virtual environment (recommended)

    conda create -n respact python=3.12
    conda activate respact
    pip install -r requirements.txt

## :hammer_and_pick: Environment Setup
## Alfworld

> [!WARNING]  
> If you are using MacOS with an arm-based system, it is recommended to use
> 
    CONDA_SUBDIR=osx-64 conda create -n alfworld python=3.12
    conda activate alfworld

Install with pip (python3.9+):

    pip install alfworld

Download PDDL & Game files and pre-trained MaskRCNN detector:
```bash
export ALFWORLD_DATA=<storage_path>
alfworld-download
```
## Run ReSpAct Experiments
```bash
export OPENAI_API_KEY=<key>
```
For Azure:
```bash
export AZURE_OPENAI_ENDPOINT=<endpoint>
export AZURE_OPENAI_KEY=<key>
```
The experiments are controlled via two yaml files, `base_config.yaml` to control the alfworld games and `experiment_configs.yaml` for the respact experimentation. 

    python main.py
    
## ðŸ›’ WebShop
## ðŸš€ Setup
Our code is implemented in Python. To setup, do the following:
1. Install [Python 3.8.13](https://www.python.org/downloads/release/python-3813/)
2. Install [Java](https://www.java.com/en/download/)
3. Download the source code:
```sh
> git clone https://github.com/princeton-nlp/webshop.git webshop
```
4. Create a virtual environment using [Anaconda](https://anaconda.org/anaconda/python) and activate it
```sh
> conda create -n webshop python=3.8.13
> conda activate webshop
```
5. Install requirements into the `webshop` virtual environment via the `setup.sh` script
```sh
> ./setup.sh [-d small|all]
```
The setup script performs several actions in the following order:
* Installs Python dependencies listed in `requirements.txt`
* Downloads product and instruction data for populating WebShop
* Downloads `spaCy en_core_web_lg` model
* Construct search engine index from product, instruction data
* Downloads 50 randomly chosen trajectories generated by MTurk workers
The `-d` flag argument allows you to specify whether you would like to pull the entire product + instruction data set (`-d all`) or a subset of 1000 random products (`-d small`).

6. By default the WebShop only loads 1,000 products for a faster environment preview. To load all products, change `web_agent_site/utils.py`:
```python
# DEFAULT_ATTR_PATH = join(BASE_DIR, '../data/items_ins_v2_1000.json')
# DEFAULT_FILE_PATH = join(BASE_DIR, '../data/items_shuffle_1000.json')
DEFAULT_ATTR_PATH = join(BASE_DIR, '../data/items_ins_v2.json')
DEFAULT_FILE_PATH = join(BASE_DIR, '../data/items_shuffle.json')
```

## ðŸ› ï¸ Usage
The WebShop environment can be rendered in two modes - `html` and `simple` - each of which offer a different observation space. The `simple` mode strips away the extraneous meta-data that the `html` mode includes to make model training and evaluation easier.
### Webpage Environment (`html` mode)
Launch the `WebShop` webpage:
```sh
> ./run_dev.sh
```
The site should then be viewable in the browser. Go to http://localhost:3000/ABC, where you should land on the search home page with a random instruction.

## Run ReSpAct Experiments
```bash
export OPENAI_API_KEY=<key>
```
For Azure:
```bash
export AZURE_OPENAI_ENDPOINT=<endpoint>
export AZURE_OPENAI_KEY=<key>
```

To-Do


## MultiWOZ
### ðŸš€ Setup
Our code is implemented in Python. To setup, do the following:
1. Go to the multiwoz folder:
```sh
> cd ./multiwoz
```
2. Create a virtual environment using [Anaconda](https://anaconda.org/anaconda/python) and activate it:
```sh
> conda create -n multiwoz python=3.10
> conda activate multiwoz
```
3. Install requirements into the `multiwoz` virtual environment:
```sh
> pip install -r requirements.txt --no-dependencies
```
4. Download the source code:
```sh
> git clone "https://github.com/smartyfh/MultiWOZ2.4.git"
```
5. Preprocess the dataset:
```sh
> cd "./MultiWOZ2.4"
> python create_data.py
> cd ".."
```
### ðŸ› ï¸ Usage
#### Offline Evaluation
For offline (traditional) evaluation, please follow the following instructions:
1. Open the `offline_evaluate.ipynb` file.
2. In the `Data Process` section, change the `sample` variable. If it is an integer, then that many of dialogues will be sampled from the test set. If it is None, then the whole test set will be used.
3. In the `Client Config` section, change the client configuration to use your own one.
4. In the `Evaluation` section, change the `save_dir` to the folder you prefer.
5. (Optional) Feel free to use your own prompt and pass it to `prompt` in the `evaluator` object.
6. (Igore the first two blocks if you have downloaded and preprocessed the dataset.) Run the `offline_evaluate.ipynb` file.
7. (Optional) Clean the generated output:
```sh
> python clean.py --file_name <generated_file_name> --new_file_name <cleaned_file_name>
```
8. Format the generated output:
```sh
> python eval_postprocess.py --file_name <file_name> --new_file_name <formatted_file_name>
```
9. Use [MultiWOZ Helper](https://github.com/uiuc-conversational-ai-lab/multiwoz-helper) to obtain the scores.
#### Online Evaluation
For offline (traditional) evaluation, please follow the following instructions:
1. Open the `online_evaluate.ipynb` file.
2. In the `Data Process` section, change the `sample` variable. If it is an integer, then that many of dialogues will be sampled from the test set. If it is None, then the whole test set will be used.
3. In the `Client Config` section, change the `client_config` for the assistant and the `user_client_config` for the user simulator to use your own ones.
4. In the `Evaluation` section, change the `save_dir` to the folder you prefer.
5. (Optional) Feel free to use your own prompt and pass it to `prompt` in the `evaluator` object.
6. (Igore the first two blocks if you have downloaded and preprocessed the dataset.) Run the `online_evaluate.ipynb` file.
7. (Optional) Clean the generated output:
```sh
> python clean.py --file_name <generated_file_name> --new_file_name <cleaned_file_name>
```
8. Format the generated output:
```sh
> python eval_postprocess_autotod.py --file_name <file_name> --new_file_name <formatted_file_name>
```
9. Use [AutoTOD](https://github.com/DaDaMrX/AutoTOD) to obtain the scores.
#### Interactive Mode
`ReAct.ipynb` provides an example for interactive mode. It supports both plain text interaction and gradio interaction.